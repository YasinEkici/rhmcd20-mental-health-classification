# train_pipeline_strategyB_v4.py

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import joblib

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import (
    StratifiedKFold,
    train_test_split,
    cross_validate,
    GridSearchCV,
    cross_val_predict
)
from sklearn.metrics import (
    accuracy_score,
    roc_auc_score,
    average_precision_score,
    classification_report,
    confusion_matrix,
    RocCurveDisplay,
    PrecisionRecallDisplay
)

# =============================================================================
# 1) CSV OKUMA ve Ã–N Ä°ÅLEMLER (Tip DÃ¶nÃ¼ÅŸÃ¼mleri + Eksik DeÄŸer Ä°mputation)
# =============================================================================

# Path nesnesiyle proje kÃ¶k dizinini belirliyoruz. 
# Path(__file__).parents[1) dosya konumunun bir Ã¼st klasÃ¶rÃ¼nÃ¼ iÅŸaret eder.
root = Path(__file__).parents[1]

# Ä°ÅŸlenmiÅŸ verinin bulunduÄŸu dosya yolunu oluÅŸturuyoruz:
data_path = root / 'data' / 'processed' / 'mental_health_finaldata_1.csv'

# Dosya yoksa hata fÄ±rlatÄ±r. BÃ¶ylece eksik dosya durumunda erken uyarÄ± alÄ±rÄ±z.
if not data_path.exists():
    raise FileNotFoundError(f"Veri dosyasÄ± bulunamadÄ±: {data_path}")

# CSV dosyasÄ±nÄ± pandas DataFrame olarak yÃ¼klÃ¼yoruz.
df = pd.read_csv(data_path)

# ---- 1.1) Ã–nce sÃ¼tunlarÄ±n dtypes'Ä±nÄ± kontrol edelim:
#      Veri okunduktan sonra hangi sÃ¼tunlarÄ±n hangi tipte olduÄŸunu ekrana yazdÄ±rÄ±yoruz.
print("ğŸ” Okunduktan sonra sÃ¼tun tipleri:\n", df.dtypes, "\n")

# ---- 1.2) NÃ¼merik olmasÄ± gereken sÃ¼tunlarÄ± sayÄ±ya Ã§evirelim (hatayÄ± NaN'a dÃ¶ndÃ¼r)
#      BazÄ± sÃ¼tunlar metin olarak veya karÄ±ÅŸÄ±k tipte gelebilir; pd.to_numeric ile hata durumunda NaN oluÅŸacak.
numeric_cols = [
    'Age',
    'Days_Indoors',
    'Growing_Stress',
    'Quarantine_Frustrations',
    'Changes_Habits',
    'Mental_Health_History',
    'Weight_Change',
    'Mood_Swings'
]
# apply(pd.to_numeric, errors='coerce') â†’ dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lemeyen hÃ¼creler NaN yapÄ±lÄ±r
df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')

# ---- 1.3) SatÄ±r bazlÄ± NaN oluÅŸumunu raporla (kaÃ§Ä± NaN oldu?)
before_drop = len(df)
# Her bir numeric sÃ¼tunda kaÃ§ NaN olduÄŸunu sayÄ±yoruz
nan_counts = df[numeric_cols].isnull().sum()
print("ğŸ” Numeric sÃ¼tunlardaki NaN sayÄ±larÄ± (dÃ¶nÃ¼ÅŸÃ¼m sonrasÄ±):\n", nan_counts, "\n")

# Bu Ã¶rnekte dropna kullanmak yerine eksik deÄŸerleri sÃ¼tun medyanÄ±yla dolduracaÄŸÄ±z.
for col in numeric_cols:
    median_val = df[col].median()             # Her sÃ¼tunun medyanÄ±nÄ± hesapla
    df[col].fillna(median_val, inplace=True)  # NaN'larÄ± medyanla doldur

# Ä°mpute sonrasÄ± toplam NaN sayÄ±sÄ±nÄ± kontrol edelim (hepsi 0 olmalÄ±)
after_impute_nan = df[numeric_cols].isnull().sum().sum()
print(f"ğŸ” TÃ¼m numeric sÃ¼tunlarda toplam NaN sayÄ±sÄ± (impute sonrasÄ±): {after_impute_nan}\n")

# ArtÄ±k NaN kalmadÄ±ÄŸÄ± iÃ§in satÄ±r kaybÄ± yaÅŸanmadÄ±. 
# SatÄ±r sayÄ±sÄ±nÄ± tekrar kontrol ediyoruz (drop iÅŸlemi yapÄ±lmadÄ±).
print("â„¹ï¸ Ä°ÅŸlem sonrasÄ± satÄ±r sayÄ±sÄ±:", len(df), "\n")


# =============================================================================
# 2) TARGET ve FEATURE AYIRMA (%80 Train+Val â€“ %20 Test)
# =============================================================================

# Target sÃ¼tununu y olarak atÄ±yoruz
y = df['Coping_Struggles']
# X, yani Ã¶zellikler kÄ±smÄ± ise 'Coping_Struggles' sÃ¼tunu dÃ¼ÅŸÃ¼lmÃ¼ÅŸ DataFrame
X = df.drop(columns=['Coping_Struggles'])

# Stratify Ã¶zelliÄŸiyle y daÄŸÄ±lÄ±mÄ±na gÃ¶re bÃ¶lerek veri kÃ¼mesini %80 train+val ve %20 test olarak ayÄ±rÄ±yoruz
X_trainval, X_test, y_trainval, y_test = train_test_split(
    X, y,
    test_size=0.20,      # %20 test
    stratify=y,          # Hedef daÄŸÄ±lÄ±mÄ±nÄ± koru
    random_state=42      # Tekrarlanabilirlik iÃ§in sabit seed
)

print(f"ğŸ” Dataset daÄŸÄ±lÄ±mÄ±:\n"
      f"  - Train+Val: {len(X_trainval)}/{len(df)} = {len(X_trainval)/len(df):.2f}\n"
      f"  - Test     : {len(X_test)}/{len(df)} = {len(X_test)/len(df):.2f}\n")


# =============================================================================
# 3) FEATURE ENGINEERING (FunctionTransformer Ä°Ã§inde)
# =============================================================================

def engineer_features(df_raw: pd.DataFrame) -> pd.DataFrame:
    """
    Bu fonksiyon, ham DataFrame'i alÄ±p yeni feature sÃ¼tunlarÄ± ekler.
    3.1) NÃ¼merik etkileÅŸim/oran/kuvvet sÃ¼tunlarÄ± ekler:
         - Mood_Days_Product: Mood_Swings * Days_Indoors
         - Weight_Med_History_Ratio: Weight_Change / (Mental_Health_History + 1)
         - Frustration_Stress_Diff: Quarantine_Frustrations - Growing_Stress
         - Stress_Sq_Frustration: Growing_Stress^2 + Quarantine_Frustrations
         - Frustration_Stress_SqRatio: (Quarantine_Frustrations + 1) / (Growing_Stress^2 + 1)
         - Weight_x_History: Weight_Change * Mental_Health_History

    3.2) Age'i kategorilere ayÄ±r:
         - Age_Group: 0-25, 26-40, 41+ aralÄ±klarÄ±nda binning yapar.

    3.3) Kategorik sÃ¼tunlarÄ± (Gender, Occupation, Work_Interest, Social_Weakness, Age_Group)
         one-hot encode (dummy Ã¶znitelik) formuna dÃ¶nÃ¼ÅŸtÃ¼r.

    3.4) Hedef sÃ¼tun (Coping_Struggles) varsa kaldÄ±r (train aÅŸamasÄ±nda zaten yok, test aÅŸamasÄ±nda da Ã¶nceden Ã§Ä±karÄ±ldÄ±).

    DÃ¶nÃ¼ÅŸ: Yeni eklenmiÅŸ sÃ¼tunlarla birlikte tamamen sayÄ±sal/kodlanmÄ±ÅŸ bir DataFrame dÃ¶ner.
    """
    # Orijinal DataFrame'i deÄŸiÅŸtirmemek iÃ§in bir kopyasÄ±nÄ± al
    df2 = df_raw.copy()

    # --- 3.1) NÃ¼merik etkileÅŸim/oran/kuvvet sÃ¼tunlarÄ± ekleme:

    # Mood_Days_Product: Mood_Swings ile Days_Indoors Ã§arpÄ±mÄ±
    df2['Mood_Days_Product'] = df2['Mood_Swings'] * df2['Days_Indoors']

    # Weight_Med_History_Ratio: Kilo deÄŸiÅŸimi / (Ruh saÄŸlÄ±ÄŸÄ± geÃ§miÅŸi + 1)
    # +1 ekleyerek bÃ¶lme sÄ±fÄ±ra bÃ¶lme hatasÄ±nÄ± Ã¶nlÃ¼yoruz.
    df2['Weight_Med_History_Ratio'] = df2['Weight_Change'] / (df2['Mental_Health_History'] + 1)

    # Frustration_Stress_Diff: Hayal kÄ±rÄ±klÄ±ÄŸÄ± - stres farkÄ±
    df2['Frustration_Stress_Diff'] = df2['Quarantine_Frustrations'] - df2['Growing_Stress']

    # Stress_Sq_Frustration: Stres karesi + hayal kÄ±rÄ±klÄ±ÄŸÄ±
    df2['Stress_Sq_Frustration'] = df2['Growing_Stress']**2 + df2['Quarantine_Frustrations']

    # Frustration_Stress_SqRatio: (Hayal kÄ±rÄ±klÄ±ÄŸÄ± + 1) / (Stres^2 + 1)
    df2['Frustration_Stress_SqRatio'] = (df2['Quarantine_Frustrations'] + 1) / (df2['Growing_Stress']**2 + 1)

    # Weight_x_History: Kilo deÄŸiÅŸimi * ruh saÄŸlÄ±ÄŸÄ± geÃ§miÅŸi
    df2['Weight_x_History'] = df2['Weight_Change'] * df2['Mental_Health_History']

    # --- 3.2) Ageâ€™i kategorilere ayÄ±ralÄ±m:
    #     Binning yapÄ±lÄ±rken bin sÄ±nÄ±rlarÄ± [0,25,40,100] kabul edildi.
    #     labels ile kategorik etiketleri belirledik.
    df2['Age_Group'] = pd.cut(
        df2['Age'],
        bins=[0, 25, 40, 100],
        labels=['0_25', '26_40', '41_plus']
    ).astype(str)  # Kategorik etiketleri string olarak saklamak iÃ§in .astype(str)

    # --- 3.3) Kategorik sÃ¼tunlar:
    categorical_cols = [
        'Gender',
        'Occupation',
        'Work_Interest',
        'Social_Weakness',
        'Age_Group'
    ]

    # --- 3.4) One-hot encode:
    #     drop_first=False, Ã§Ã¼nkÃ¼ bir kategori referans olarak dÃ¼ÅŸÃ¼rmek istemiyoruz; 
    #     tÃ¼m dummy sÃ¼tunlarÄ±nÄ± ayrÄ± ayrÄ± tutacaÄŸÄ±z.
    df_encoded = pd.get_dummies(df2, columns=categorical_cols, drop_first=False)

    # --- 3.5) Hedef sÃ¼tun hÃ¢lÃ¢ varsa kaldÄ±ralÄ±m
    df_encoded = df_encoded.drop(columns=['Coping_Struggles'], errors='ignore')

    # Yeni eklenen tÃ¼m sÃ¼tunlarla birlikte DataFrame'i dÃ¶ndÃ¼r
    return df_encoded

# FunctionTransformer: yukarÄ±daki engineer_features fonksiyonunu pipeline iÃ§inde kullanabilmek iÃ§in sarÄ±yoruz.
fe_transformer = FunctionTransformer(engineer_features, validate=False)


# =============================================================================
# 4) PIPELINE ve HYPERPARAMETER GRID
# =============================================================================

# Pipeline adÄ±mlarÄ±:
#  - 'fe' kÄ±smÄ±nda: engineer_features fonksiyonu uygulanacak
#  - 'clf' kÄ±smÄ±nda: DecisionTreeClassifier (sabit random_state ile) 
pipe = Pipeline([
    ('fe', fe_transformer),
    ('clf', DecisionTreeClassifier(random_state=42))
])

# DecisionTreeClassifier iÃ§in ayarlanacak hiperparametre Ä±zgarasÄ±
param_grid = {
    'clf__max_depth': [3, 5, 7],             # AÄŸacÄ±n maksimum derinliÄŸi
    'clf__min_samples_leaf': [2, 5, 10],     # Her yaprakta en az kaÃ§ Ã¶rnek olmalÄ±
    'clf__min_samples_split': [2, 5, 10],    # Bir dÃ¼ÄŸÃ¼m kaÃ§ Ã¶rnekte split edilmeli
    'clf__ccp_alpha': [0.0, 0.0001],         # Cost-complexity pruning parametresi
    'clf__criterion': ['gini', 'entropy'],   # BÃ¶lÃ¼nme Ã¶lÃ§Ã¼tÃ¼
    'clf__class_weight': [None, 'balanced']  # SÄ±nÄ±f dengesizliÄŸi varsa dengeli aÄŸÄ±rlÄ±k
}


# =============================================================================
# 5) NESTED CV: 5-fold Outer, 3-fold Inner
# =============================================================================

# DÄ±ÅŸ dÃ¶ngÃ¼ (outer) iÃ§in StratifiedKFold: 5 katlÄ±, shuffle=True, random_state sabit
outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
# Ä°Ã§ dÃ¶ngÃ¼ (inner) iÃ§in StratifiedKFold: 3 katlÄ±, shuffle=True, random_state sabit
inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# DeÄŸerlendirme metrikleri: accuracy, roc_auc, average_precision
scoring = ['accuracy', 'roc_auc', 'average_precision']

# cross_validate ile nested CV gerÃ§ekleÅŸtiriyoruz.
# Ref: https://scikit-learn.org/stable/modules/cross_validation.html#nested-cross-validation
nested_cv_results = cross_validate(
    estimator=GridSearchCV(
        estimator=pipe,          # Pipeline: fe + DecisionTree
        param_grid=param_grid,   # Hyperparametre Ä±zgarasÄ±
        cv=inner_cv,             # Ä°Ã§ dÃ¶ngÃ¼: 3-fold CV
        scoring='roc_auc',       # Ä°Ã§ dÃ¶ngÃ¼ optimizasyonu ROC-AUC Ã¼zerinden
        n_jobs=-1,               # Paralel hesaplama (tÃ¼m Ã§ekirdekleri kullan)
        refit=False              # SonuÃ§larÄ± doÄŸrudan refit etme (dÄ±ÅŸ dÃ¶ngÃ¼ sonuÃ§larÄ±na bakacaÄŸÄ±z)
    ),
    X=X_trainval,                # Train+Val verisi
    y=y_trainval,                # Train+Val hedef
    cv=outer_cv,                 # DÄ±ÅŸ dÃ¶ngÃ¼: 5-fold CV
    scoring=scoring,             # DÄ±ÅŸ dÃ¶ngÃ¼de Ã¶lÃ§mek istediÄŸimiz metrikler
    return_estimator=True,       # Her outer katman sonrasÄ± estimator nesnelerini geri dÃ¶n
    n_jobs=-1                    # Paralel hesaplama
)

# Her bir fold iÃ§in test_accuracy, test_roc_auc, test_average_precision deÄŸerlerini yazdÄ±r
for i in range(len(nested_cv_results['test_accuracy'])):
    print(f"Fold {i+1}: "
          f"Acc = {nested_cv_results['test_accuracy'][i]:.3f}, "
          f"ROC-AUC = {nested_cv_results['test_roc_auc'][i]:.3f}, "
          f"PR-AUC = {nested_cv_results['test_average_precision'][i]:.3f}"
    )

print("\nğŸ”„ Out-of-Fold Ortalama SonuÃ§lar:")
print(f"  Accuracy : {np.mean(nested_cv_results['test_accuracy']):.3f}")
print(f"  ROC-AUC   : {np.mean(nested_cv_results['test_roc_auc']):.3f}")
print(f"  PR-AUC    : {np.mean(nested_cv_results['test_average_precision']):.3f}\n")


# =============================================================================
# 6) EN Ä°YÄ° PARAMETRELER VE FÄ°NAL PIPE
# =============================================================================

# ROC-AUC'ya gÃ¶re en yÃ¼ksek skoru veren fold'u buluyoruz
best_fold_index = np.argmax(nested_cv_results['test_roc_auc'])
# Ä°lgili fold'un GridSearchCV nesnesini alÄ±yoruz
best_inner_gs = nested_cv_results['estimator'][best_fold_index]
# EÄŸer best_inner_gs iÃ§inde best_params_ varsa Ã§ek, yoksa None
best_params = best_inner_gs.best_params_ if hasattr(best_inner_gs, 'best_params_') else None

if best_params is None:
    print("âš ï¸ Ä°Ã§ CVâ€™den best_params alÄ±namadÄ± (refit=False).")
    print("   â†’ Final aÅŸamada tÃ¼m trainval Ã¼zerinde yeniden GridSearchCV yapÄ±lacak.\n")
else:
    print(f"ğŸ“Œ Nested CVâ€™den elde edilmiÅŸ Ã¶rnek best_params: {best_params}\n")

# Final aÅŸamada Pipeline'Ä± yeniden oluÅŸturup, trainval verisiyle GridSearchCV yapacaÄŸÄ±z
final_pipe = Pipeline([
    ('fe', fe_transformer),
    ('clf', DecisionTreeClassifier(random_state=42))
])
final_grid = GridSearchCV(
    estimator=final_pipe,
    param_grid=param_grid,
    cv=5,                    # 5-fold CV (trainval Ã¼zerinde)
    scoring='roc_auc',       # ROC-AUC optimizasyonu
    n_jobs=-1                # Paralel hesaplama
)
# Train+Val verisi Ã¼zerinde en iyi parametreleri bul
final_grid.fit(X_trainval, y_trainval)

print("ğŸ“Œ Final aÅŸamada (Train+Val tÃ¼mÃ¼) seÃ§ilen best_params (StratejiB_v4):")
print(final_grid.best_params_, "\n")

# En iyi tahmin eden pipeline Ã¶rneÄŸini best_pipe olarak alalÄ±m
best_pipe = final_grid.best_estimator_

# =============================================================================
# 7) OUT-OF-FOLD (OOF) ROC & PR EÄRÄ°LERÄ°
# =============================================================================

# cross_val_predict ile trainval Ã¼zerinde out-of-fold predict_proba deÄŸerlerini elde ediyoruz.
# method='predict_proba' â†’ tahmin edilen olasÄ±lÄ±k skorlarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r
y_prob_oof = cross_val_predict(
    estimator=best_pipe,
    X=X_trainval,
    y=y_trainval,
    cv=outer_cv,                # DÄ±ÅŸ dÃ¶ngÃ¼ (aynÄ± outer_cv kullanÄ±lÄ±yor)
    method='predict_proba',     # OlasÄ±lÄ±k tahmini
    n_jobs=-1
)[:, 1]                        # Pozitif sÄ±nÄ±fÄ±n (1) olasÄ±lÄ±k skorlarÄ±

print("ğŸ”„ Out-of-Fold ROC & PR eÄŸrileri:")
# ROC eÄŸrisini Ã§iziyoruz (trainval OOF sonuÃ§larÄ±)
RocCurveDisplay.from_predictions(y_trainval, y_prob_oof).plot()
plt.title("ROC Curve (Out-of-Fold)")
plt.show()

# Precision-Recall eÄŸrisini Ã§iziyoruz (trainval OOF sonuÃ§larÄ±)
PrecisionRecallDisplay.from_predictions(y_trainval, y_prob_oof).plot()
plt.title("Precision-Recall Curve (Out-of-Fold)")
plt.show()


# =============================================================================
# 8) TEST SET PERFORMANSI
# =============================================================================

# Test set Ã¼zerinde doÄŸrudan predict ve predict_proba ile tahminleri alÄ±yoruz:
y_pred_test = best_pipe.predict(X_test)
y_prob_test = best_pipe.predict_proba(X_test)[:, 1]  # OlasÄ±lÄ±k skoru (pozitif sÄ±nÄ±f)

print("=== TEST SET PerformansÄ± ===")
# SÄ±nÄ±flandÄ±rma raporu (precision, recall, f1-score, destek sayÄ±sÄ±)
print(classification_report(y_test, y_pred_test))
# Test set ROC-AUC skoru
print(f"Test ROC-AUC: {roc_auc_score(y_test, y_prob_test):.3f}")
# Test set PR-AUC skoru (Average Precision)
print(f"Test PR-AUC  : {average_precision_score(y_test, y_prob_test):.3f}\n")

# ROC eÄŸrisini Test set iÃ§in Ã§iz
RocCurveDisplay.from_predictions(y_test, y_prob_test).plot()
plt.title("ROC Curve (Test Set)")
plt.show()

# Precision-Recall eÄŸrisini Test set iÃ§in Ã§iz
PrecisionRecallDisplay.from_predictions(y_test, y_prob_test).plot()
plt.title("Precision-Recall Curve (Test Set)")
plt.show()

# KarÄ±ÅŸÄ±klÄ±k matrisi (confusion matrix) Ã§izimi:
cm = confusion_matrix(y_test, y_pred_test)
fig, ax = plt.subplots(figsize=(4, 4))
im = ax.matshow(cm, cmap=plt.cm.Blues)
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
ax.set_xticks([0, 1])
ax.set_yticks([0, 1])
plt.title("Confusion Matrix (Test)")
plt.colorbar(im, ax=ax)
plt.show()


# =============================================================================
# 9) FEATURE IMPORTANCES (Ã–nemli Ã–zellikler)
# =============================================================================

# DecisionTreeClassifier adÄ±mÄ±nÄ± Ã§Ä±kartÄ±p feature_importances_ Ã¶zelliÄŸini alÄ±yoruz
clf_final = best_pipe.named_steps['clf']
# Test seti Ã¼zerinde Feature Engineering uygulanmÄ±ÅŸ haliyle X_test'i elde etmeliyiz
X_test_fe = engineer_features(X_test)
# Feature isimleri, DataFrame sÃ¼tun adlarÄ±ndan alÄ±nÄ±r
feature_names = X_test_fe.columns.to_list()

# Modelin attribute'u olan feature_importances_ dizisini kullanÄ±yoruz
importances = clf_final.feature_importances_
# En yÃ¼ksek 10 Ã¶zelliÄŸin indekslerini alÄ±yoruz (kÃ¼Ã§Ã¼kten bÃ¼yÃ¼ÄŸe sÄ±ralayÄ±p son 10'u dilimliyoruz)
indices = np.argsort(importances)[-10:]

# Ã‡ubuk grafik (barh) Ã§iziyoruz: yatay barlar en yÃ¼ksek 10 Ã¶nemli feature iÃ§in
fig, ax = plt.subplots(figsize=(6, 4))
ax.barh([feature_names[i] for i in indices], importances[indices])
ax.set_title("Top 10 Feature Importances")
ax.set_xlabel("Importance")
plt.tight_layout()
plt.show()


# =============================================================================
# 10) MODELÄ° KAYDET
# =============================================================================

# best_pipe pipeline'Ä±nÄ± joblib ile diske kaydediyoruz, bÃ¶ylece yeniden eÄŸitim yapmadan yÃ¼kleyebiliriz.
model_path = root / 'models' / 'decision_tree_stratB_v4.pkl'
joblib.dump(best_pipe, model_path)
print(f"\nğŸ’¾ Final pipeline (StratejiB_v4) kaydedildi: {model_path}\n")
