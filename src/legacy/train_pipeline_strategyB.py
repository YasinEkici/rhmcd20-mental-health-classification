#!/usr/bin/env python3
"""
train_pipeline_strategyB.py

Strateji B: Nested Cross-Validation (iÃ§â€dÄ±ÅŸ CV) ile hem model seÃ§imi/pruning (iÃ§ CV) 
hem de genelleme performansÄ± tahmini (dÄ±ÅŸ CV) yapar. 
Tek bir holdâ€out test setâ€™i ayÄ±rmak yerine, tÃ¼m veri Ã¼zerinde nested CV gerÃ§ekleÅŸtirir.

AdÄ±m adÄ±m:
 1) Ham veri yÃ¼klenir, preprocess_data ile Ã¶n iÅŸleme (ordinal mapping, one-hot, target encode).
 2) X/y ayrÄ±lÄ±r.
 3) Pipeline: engineer_features + DecisionTree (henÃ¼z parametre yok).
 4) Ä°Ã§ CV: GridSearchCV ile pruning ve parametre seÃ§imi (Ã¶r. max_depth, min_samples_leaf, ccp_alpha).
 5) DÄ±ÅŸ CV: cross_val_score kullanarak her foldâ€™da â€œiÃ§inde GridSearchâ€ iÅŸlemi yapÄ±lÄ±r ve o fold test edilir.
 6) Elde edilen nested CV doÄŸruluk skorlarÄ±, gerÃ§ekÃ§i genelleme performansÄ± verir.
 7) (Opsiyonel) En iyi parametre kombinasyonu tek seferde seÃ§ip, tÃ¼m veriyle final model eÄŸitilebilir ve kaydedilebilir.

KullanÄ±m:
  python train_pipeline_strategyB.py
  python train_pipeline_strategyB.py --save-model
"""

import argparse
from pathlib import Path

import pandas as pd
import joblib
import numpy as np

from sklearn.pipeline      import Pipeline
from sklearn.preprocessing import FunctionTransformer
from sklearn.model_selection import (
    KFold,
    GridSearchCV,
    cross_val_score
)
from sklearn.metrics       import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    average_precision_score,
    RocCurveDisplay,
    PrecisionRecallDisplay
)
from sklearn.tree          import DecisionTreeClassifier
import matplotlib.pyplot as plt

from legacy.preprocessing2       import preprocess_data
from feature_engineering import engineer_features

# Sabitler
ROOT_DIR = Path(__file__).parents[1]
RAW_PATH = ROOT_DIR / 'data' / 'raw' / 'mental_health_finaldata_1.csv'
MODEL_DIR = ROOT_DIR / 'models'
MODEL_DIR.mkdir(parents=True, exist_ok=True)


def plot_roc_pr(y_true, y_prob, title_suffix=""):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    RocCurveDisplay.from_predictions(y_true, y_prob, ax=ax1)
    ax1.set_title(f"ROC Curve {title_suffix}")
    PrecisionRecallDisplay.from_predictions(y_true, y_prob, ax=ax2)
    ax2.set_title(f"Precision-Recall Curve {title_suffix}")
    plt.tight_layout()
    plt.show()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--save-model',
        action='store_true',
        help='TÃ¼m veriyle en iyi parametreleri bulup fit edilen final modeli kaydet'
    )
    args = parser.parse_args()

    # ========== 1) Veri yÃ¼kle & preprocess ==========
    df_raw = pd.read_csv(RAW_PATH)
    df_proc = preprocess_data(df_raw)  # ordinal mapping, one-hot, target map
    X = df_proc.drop('Coping_Struggles', axis=1)
    y = df_proc['Coping_Struggles']

    # ========== 2) Pipeline tanÄ±mÄ± ==========
    base_pipeline = Pipeline([
        ('fe',  FunctionTransformer(engineer_features, validate=False)),
        ('clf', DecisionTreeClassifier(random_state=42, class_weight='balanced'))
    ])

    # ========== 3) Parametre Ä±zgarasÄ± ==========
    param_grid = {
        'clf__max_depth':        [5, 10, 15, None],
        'clf__min_samples_leaf': [5, 10, 20, 50],
        'clf__min_samples_split':[10, 20, 50, 100],
        'clf__criterion':        ['gini', 'entropy'],
        'clf__ccp_alpha':        [0.0, 0.001, 0.01]
    }

    # ========== 4) Nested CV ==========
    #  - cv_inner: parametre seÃ§imi (GridSearch) iÃ§in
    #  - cv_outer: gerÃ§ek genelleme performansÄ± iÃ§in
    cv_inner = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_outer = KFold(n_splits=5, shuffle=True, random_state=42)

    grid_search = GridSearchCV(
        estimator=base_pipeline,
        param_grid=param_grid,
        cv=cv_inner,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )

    print("ğŸ”„ Nested CV baÅŸlÄ±yor (iÃ§ CV ile GridSearch ve dÄ±ÅŸ CV ile skor)...")
    # cross_val_score, her outer foldâ€™da:
    #   1) Ä°Ã§ foldâ€™larda grid_search.fit(...) Ã§alÄ±ÅŸtÄ±rÄ±r
    #   2) En iyi estimator ile outer foldâ€™un test kÄ±smÄ±nda predict eder
    nested_scores = cross_val_score(
        grid_search, X, y,
        cv=cv_outer,
        scoring='accuracy',
        n_jobs=-1
    )
    print(f"\nNested CV accuracies (5 outer folds): {nested_scores}")
    print(f"Nested CV mean accuracy             : {nested_scores.mean():.4f}")

    # ========== 5) (Opsiyonel) TÃ¼m veriyle en iyi parametre seÃ§imi ve eÄŸitim ==========
    if args.save_model:
        print("\nğŸ”§ TÃ¼m veriyle en iyi parametreleri bulmak iÃ§in grid search yapÄ±lÄ±yor...")
        # Tekrar grid search, tÃ¼m veri Ã¼zerinde:
        grid_search.fit(X, y)
        print(f"En iyi parametreler (tÃ¼m veri): {grid_search.best_params_}")
        best_pipeline = grid_search.best_estimator_
        # Best pipeline zaten iÃ§inde FE ve best DT parametrelerini tutuyor
        # Tekrar tÃ¼m veriyle fit edilmiÅŸ durumda:
        best_pipeline.fit(X, y)

        # Modeli kaydet
        save_path = MODEL_DIR / 'decision_tree_stratB.pkl'
        joblib.dump(best_pipeline, save_path)
        print(f"ğŸ’¾ Final pipeline (StratejiB) kaydedildi --> {save_path}")

        # AyrÄ±ca tÃ¼m veri Ã¼zerindeki ROC/PR skorlarÄ±nÄ± da gÃ¶sterebiliriz:
        y_prob_all = best_pipeline.predict_proba(X)[:, 1]
        roc_all = roc_auc_score(y, y_prob_all)
        pr_all  = average_precision_score(y, y_prob_all)
        print(f"TÃ¼m veri ROC-AUC (overfit check): {roc_all:.3f}")
        print(f"TÃ¼m veri PR-AUC  (overfit check): {pr_all:.3f}")
        plot_roc_pr(y, y_prob_all, title_suffix="(TÃ¼m Veri)")

    # Nested CVâ€™nun dÄ±ÅŸÄ±nda tekrar classification report vs. yazmaya gerek yok,
    # Ã§Ã¼nkÃ¼ nested_scores zaten â€œgerÃ§ekÃ§i genelleme hatasÄ±nÄ±nâ€ bir Ã¶zetini verdi.
    # EÄŸer istersek, cross_val_predict ile outâ€ofâ€fold tahminleri alÄ±p ROC/PR Ã§izebiliriz:
    from sklearn.model_selection import cross_val_predict

    print("\nğŸ”„ Out-of-Fold tahminleri ile ROC/PR Ã§iziliyor...")
    oof_probs = cross_val_predict(
        base_pipeline, X, y,
        cv=cv_outer,
        method='predict_proba',
        n_jobs=-1
    )[:, 1]

    roc_oof = roc_auc_score(y, oof_probs)
    pr_oof  = average_precision_score(y, oof_probs)
    print(f"Out-of-Fold ROC-AUC: {roc_oof:.3f}")
    print(f"Out-of-Fold PR-AUC : {pr_oof:.3f}")
    plot_roc_pr(y, oof_probs, title_suffix="(Out-of-Fold)")

    # Opsiyonel: EÄŸer istersen, tÃ¼m veri Ã¼zerinde parametre seÃ§ip final eÄŸitim yaptÄ±ktan sonra,
    # test etmek iÃ§in farklÄ± bir veri geldiÄŸinde (production), doÄŸrudan   best_pipeline.predict(new_X)   diyebilirsin.
    # Burada test set ayrÄ± olmadÄ±ÄŸÄ± iÃ§in â€œproduction testâ€ Ã¶rneÄŸi yok.

if __name__ == '__main__':
    main()
